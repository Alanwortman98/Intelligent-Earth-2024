{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ce5e4a5-9988-4500-836d-0a2e6b88100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ea4f4-f265-427d-89ff-45cfdcf96fc7",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "02df2a2f-402c-40a2-87d0-686a63d0e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization is poor for relu\n",
    "\n",
    "def initialize_params(dims): # He initialization ; for ReLu and Leaky ReLu activations\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dims - python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters - python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    params = {}\n",
    "    for i in range(1, len(dims)):\n",
    "        params.update({f'W{i}': np.random.randn(dims[i],dims[i-1]) * np.sqrt(2 / dims[i])}) # scaled initialization for ReLU\n",
    "        params.update({f'B{i}': np.zeros((dims[i], 1))})\n",
    "\n",
    "    assert(params['W' + str(i)].shape == (dims[i], dims[i-1])) # double check dimensions are correct\n",
    "    assert(params['B' + str(i)].shape == (dims[i], 1))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b48d0-817e-4dec-97c4-c35384069e88",
   "metadata": {},
   "source": [
    "### Define activation functions\n",
    "\n",
    "Using ReLu for this assignment. \n",
    "\n",
    "Several others are included which I would like to fill in later for my own understanding/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f7dcca-9f10-466d-b892-219da2d21541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "Z - pre activation parameter ; output of the linear layer\n",
    "\n",
    "Returns:\n",
    "A - post activation parameter ; of the same shape as Z\n",
    "cache - returns Z ; used during backpropagation\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z)) # the sigmoid function\n",
    "    dZ = dA * (s * (1-s)) # dA * the derivative of sigmoid. sigmoid derivative = (e ** -x) /(1 + e ** -X) = sig(x) * (1 - sig(x))\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    dZ[Z <= 0] = 0 # When z <= 0, you should set dz to 0 as well - think of what ReLu looks like. \n",
    "\n",
    "    assert (dZ.shape == Z.shape) # check if dZ has correct shape\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def tanh(Z):\n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    return A, cache\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    Z = cache\n",
    "    t = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z)) # the tanh function\n",
    "    dZ = dA * (1 - t ** 2) # dA * dervative of tanh. \n",
    "\n",
    "    assert (dZ.shape == Z.shape) # check if dZ has correct shape\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def softplus(Z):\n",
    "    A = np.log(1 + np.exp(Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softplus_backward(dA, cache): # define later\n",
    "    pass\n",
    "\n",
    "def softmax(Z): # really only used in the output layer\n",
    "    A = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.exp(Z - np.max(Z, axis=1, keepdims=True)).sum(axis=1, keepdims=True) # numerically stable softmax\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backeard(dA, cache): # define later\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c921f-0e58-40d6-bcca-e44f1eb82e17",
   "metadata": {},
   "source": [
    "### Feed Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c9f85be1-0e51-4148-a3c2-ffab888fe9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward (A, W, b): # linear transformation\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A - activations from previous layer: numpy array of shape (size of previous layer, number of examples)\n",
    "    W - weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b - bias vector: numpy array of shape (size of current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z - input of the activation function (pre-activation parameter)\n",
    "    cache - tuple: (A, W, b) ; used for computing backwards pass\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b  \n",
    "    cache = (A, W, b) \n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1])) # double check dimensions are correct (W rows, A columns)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "483df950-4211-4026-8866-687283fe0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, weights, biases, activation): # linear transformation with activation function\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A_prev - activations from previous layer: numpy array of shape (size of previous layer, number of examples)\n",
    "    weights - weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    biases - bias vector: numpy array of shape (size of current layer, 1)\n",
    "    activation - the activation function to be used: string\n",
    "\n",
    "    Returns:\n",
    "    A_new - output of activation function: \n",
    "    cache - tuple: (linear cache, activation cache) ; used for computing backwards pass\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, weights, biases)\n",
    "\n",
    "    if activation == \"tanh\":\n",
    "        A_new, activation_cache = tanh(Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        A_new, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        A_new, activation_cache = relu(Z)\n",
    "\n",
    "    elif activation == \"softplus\":\n",
    "        pass\n",
    "\n",
    "    assert (A_new.shape == (weights.shape[0] , A_prev.shape[1])) # (W rows, A cols)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A_new, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4e19496f-0fdb-40a4-a66e-f325f89363b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(X, parameters): # feed forward\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X - data to be modeled: numpy array of shape (input size, number of examples)\n",
    "    parameters - python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "\n",
    "    Returns:\n",
    "    A_final - activations from final layer\n",
    "    caches - an indexed list of caches from every layer containing the previous activations, weights, biases, and pre-activation parameters \n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # number of layers in the model\n",
    "\n",
    "    for i in range(1, L): # linear --> relu for layers 1-->(L-1)\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[f'W{i}'], parameters[f'B{i}'], \"relu\") # hidden layers # note: relu is hard coded\n",
    "        caches.append(cache)\n",
    "\n",
    "    # linear transformation for last layer (regression model)\n",
    "    A_L, cache = linear_forward(A, parameters[f'W{L}'], parameters[f'B{L}']) # output layer # note: for regression, usually no activation (or a linear transformation) is used.\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(A_L.shape == (1, X.shape[1])) # double check dimensions are correct\n",
    "\n",
    "    return A_L, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205b57e-7188-4610-8a9e-0957a46c9368",
   "metadata": {},
   "source": [
    "### Define some loss functions\n",
    "\n",
    "Will be using MSE in this assignment  \n",
    "Several others are included which I would like to fill in later for my own understanding/research  \n",
    "### Dot Product Formula\n",
    "\n",
    "The dot product of two vectors \\( \\mathbf{A} \\) and \\( \\mathbf{B} \\) is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i \\cdot B_i\n",
    "$$\n",
    "\n",
    "### Mean Squared Error (MSE) Formula\n",
    "\n",
    "The Mean Squared Error (MSE) is calculated as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c3b06e4f-16c5-4568-abe0-7b29e5782337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "Returns:\n",
    "cost -- cross-entropy cost\n",
    "\"\"\"\n",
    "\n",
    "def binary_cross_entropy(): # log loss ; classification\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)) # using dot product instead of np.sum() becasue it's faster\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def categorical_cross_entropy(): # log loss ; classification\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * np.sum(np.mulitply(Y, np.log(AL))) # this is unfinished and I'm not sure if it's correct\n",
    "\n",
    "    return cost\n",
    "\n",
    "def neg_log_likelihood(): # for simple classifications\n",
    "    pass\n",
    "\n",
    "def MSE(AL, Y): # mean squared error ; regression\n",
    "    #m = Y.shape[1]\n",
    "    m = Y.shape[0]\n",
    "    cost = (1 / m) * np.dot((AL-Y).flatten(), (AL-Y).flatten()) # using dot product instead of np.sum() because it's faster\n",
    "                                                                # does anything need to be trasposed?\n",
    "    return cost\n",
    "\n",
    "def MAE(): # mean absolute error ; regression\n",
    "    pass\n",
    "\n",
    "def KL_divergence(): # Kullback-Leibler Divergence\n",
    "    pass\n",
    "\n",
    "def Huber(): # Huber loss ; regression\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e2882-d710-41a4-8e32-a458ceb541af",
   "metadata": {},
   "source": [
    "### Define a backprop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6fde9e-1931-42b0-9f36-64d9e6b4e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can the autodiff assignment be used here?\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dZ - Gradient of the cost with respect to the linear output (of the current layer)\n",
    "    cache - tuple ; (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev - Gradient of the cost with respect to the activation (of the previous layer) ; same shape as A_prev\n",
    "    dW - Gradient of the cost with respect to W (of the current layer) ; same shape as W\n",
    "    db - Gradient of the cost with respect to b (of the current layer) ; same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1] # is this the length of entries?\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ) # what's happening here?\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T) # what's happening here?\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims = True) # what's happening here?\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape) \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "417f3eea-58af-45d6-8fe4-84f00817d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    activation function takes input Z and returns A and an activation cache (Z) ; \n",
    "    activation_backward function takes input dA and the activation cache (Z) and returns dZ.\n",
    "    \n",
    "    Args:\n",
    "    dA - post-activation gradient for current layer\n",
    "    cache - tuple ; (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation - activation function used in this layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev - Gradient of the cost with respect to the activation (of the previous layer) ; same shape as A_prev\n",
    "    dW - Gradient of the cost with respect to W (of the current layer) ; same shape as W\n",
    "    db - Gradient of the cost with respect to b (of the current layer) ; same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\": # relu activation takes input Z and returns A and an activation cache (Z) ; relu_backward takes input dA and the activation cache (Z) and returns dZ\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dA_prev, dW, b = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b9724910-61a1-4a1d-bc53-0f59c2843f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    AL - probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y - true \"label\" vector\n",
    "    caches - list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "                how would this work for tanh? softplus?\n",
    "\n",
    "    Returns:\n",
    "    grads - dictionary ; gradients with respect to activations, weights, biases...\n",
    "                         grads[\"dA\" + str(l)] = ... \n",
    "                         grads[\"dW\" + str(l)] = ...\n",
    "                         grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    # m = AL.shape[1]\n",
    "    m = AL.shape[0]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # initialize backprop\n",
    "\n",
    "    # current_cache = caches[L-1] # Last Layer\n",
    "    current_cache = caches[L-2] # b/c no activation cache in last layer since no activation function was used\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "        dAL, \n",
    "        current_cache, \n",
    "        activation = \"relu\") # note: relu is hard coded. This is the last (output) layer\n",
    "\n",
    "    for i in reversed(range(L-1)): # Loop from l=L-2 to l=0\n",
    "        current_cache = caches[i]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
    "            grads[\"dA\" + str(i+1)],\n",
    "            current_cache,\n",
    "            activation = \"relu\") # note: relu is hard coded. These are the hidden layers\n",
    "\n",
    "        grads[\"dA\" + str(i)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(i+1)] = dW_temp\n",
    "        grads[\"db\" + str(i+1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b8dab-f8d4-40d3-9525-724a78cf8be3",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "which gradient descent update rule to use? \n",
    "\n",
    "which learning rate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b3df7f-1267-4772-a9a4-d7d566938e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_params(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    generally we update by: new_params = params - grads * learning_rate\n",
    "    Args:\n",
    "    params - dict ; contains the (previous) parameters\n",
    "    grads - dict ; contains the graduients calculated from backprop\n",
    "    learning_rate - step size for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params - dict ; the updated parameters\n",
    "                parameters[\"W\" + str(l)] = ... \n",
    "                parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(params) // 2 # params dict contains entries for both weights and biases\n",
    "\n",
    "    for i in range(L):\n",
    "        params[\"W\" + str(i+1)] = params[\"W\" + str(i+1)] - grads[\"dW\" + str(i+1)] * learning_rate\n",
    "        params[\"b\" + str(i+1)] = params[\"b\" + str(i+1)] - grads[\"db\" + str(i+1)] * learning_rate\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c3444-9a23-4686-ba11-fbc9142a3a06",
   "metadata": {},
   "source": [
    "### Import a Dataset for regression\n",
    "\n",
    "There are 14 attributes in each case of the dataset. They are:  \n",
    "CRIM - per capita crime rate by town  \n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.  \n",
    "INDUS - proportion of non-retail business acres per town.  \n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)  \n",
    "NOX - nitric oxides concentration (parts per 10 million)  \n",
    "RM - average number of rooms per dwelling  \n",
    "AGE - proportion of owner-occupied units built prior to 1940  \n",
    "DIS - weighted distances to five Boston employment centres  \n",
    "RAD - index of accessibility to radial highways  \n",
    "TAX - full-value property-tax rate per ${$10,000} $  \n",
    "PTRATIO - pupil-teacher ratio by town  \n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town  \n",
    "LSTAT - % lower status of the population  \n",
    "MEDV - Median value of owner-occupied homes in \\$1000's  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c09bb97b-d660-452c-a682-7818411d280a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Housing = pd.read_csv(\"BostonHousing.csv\")\n",
    "Housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9478135-2927-4424-817e-ef233a832423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data set into numpy. Goal is to predict the age of a person based on the other attributes for housing\n",
    "\n",
    "features = Housing.columns.drop('age').to_list() \n",
    "\n",
    "inputs = Housing[features].to_numpy() # input features\n",
    "targets = Housing.age.to_numpy() # target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "aa479296-1d8c-42fd-a0c3-a694f811ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set up into training sets and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, targets, test_size = 0.2, random_state = 0)\n",
    "\n",
    "X_train = X_train.T # transpose to have 13 rows (for features) and 406 columns (for data points)\n",
    "X_test = X_test.T # this makes things compatible with the layout of the weights matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "633d1d86-c663-46b1-a243-c54dec25111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number layers and nodes in each\n",
    "# initialize parameters\n",
    "\n",
    "dimensions = [13,8,5,1] # the layout of the neural network\n",
    "# input layer must have dims of # of features. output layer must have dim of 1 for regression (multiple for multiclass classification)\n",
    "\n",
    "params = initialize_params(dimensions) # initial weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "10d7e52b-a735-40bc-9a7e-3c89e2ad51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_L, cache_forward = model_forward(X_train, params)\n",
    "A_L = A_L.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b40e03a6-b9f2-472e-91f4-1dc930e66252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702710.0465814223\n"
     ]
    }
   ],
   "source": [
    "error = MSE(A_L, Y_train)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8eb3d8b9-1456-4c89-a2db-05f6107e97fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[324], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m AL, Y_assess, caches \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_L\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[323], line 28\u001b[0m, in \u001b[0;36mmodel_backward\u001b[1;34m(AL, Y, caches)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# current_cache = caches[L-1] # Last Layer\u001b[39;00m\n\u001b[0;32m     27\u001b[0m current_cache \u001b[38;5;241m=\u001b[39m caches[L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;66;03m# b/c no activation cache in last layer since no activation function was used\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)] \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_activation_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# note: relu is hard coded. This is the last (output) layer\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)): \u001b[38;5;66;03m# Loop from l=L-2 to l=0\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     current_cache \u001b[38;5;241m=\u001b[39m caches[i]\n",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m, in \u001b[0;36mlinear_activation_backward\u001b[1;34m(dA, cache, activation)\u001b[0m\n\u001b[0;32m     17\u001b[0m linear_cache, activation_cache \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m# relu activation takes input Z and returns A and an activation cache (Z) ; relu_backward takes input dA and the activation cache (Z) and returns dZ\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     dZ \u001b[38;5;241m=\u001b[39m \u001b[43mrelu_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     23\u001b[0m     dZ \u001b[38;5;241m=\u001b[39m tanh_backward(dA, activation_cache)\n",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m, in \u001b[0;36mrelu_backward\u001b[1;34m(dA, cache)\u001b[0m\n\u001b[0;32m     30\u001b[0m Z \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m     31\u001b[0m dZ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(dA, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# just converting dz to a correct object.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mdZ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# When z <= 0, you should set dz to 0 as well - think of what ReLu looks like. \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (dZ\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m Z\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# check if dZ has correct shape\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dZ\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = model_backward(A_L, Y_train, cache_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "928b1eb9-95d2-4dc0-bece-3486ba75b7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cache_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4941b704-e294-4698-bb85-fa7d21fa3a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((array([[3.5809e-01, 1.5876e-01, 1.1329e-01, ..., 1.5098e-01, 2.2927e-01,\n",
       "           1.3914e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 3.0000e+01, ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          [6.2000e+00, 1.0810e+01, 4.9300e+00, ..., 1.0010e+01, 6.9100e+00,\n",
       "           4.0500e+00],\n",
       "          ...,\n",
       "          [3.9170e+02, 3.7694e+02, 3.9125e+02, ..., 3.9451e+02, 3.9274e+02,\n",
       "           3.9690e+02],\n",
       "          [9.7100e+00, 9.8800e+00, 1.1380e+01, ..., 1.0300e+01, 1.8800e+01,\n",
       "           1.4690e+01],\n",
       "          [2.6700e+01, 2.1700e+01, 2.2000e+01, ..., 1.9200e+01, 1.6600e+01,\n",
       "           2.3100e+01]]),\n",
       "   array([[ 0.88202617,  0.2000786 ,  0.48936899,  1.1204466 ,  0.933779  ,\n",
       "           -0.48863894,  0.47504421, -0.0756786 , -0.05160943,  0.20529925,\n",
       "            0.07202179,  0.72713675,  0.38051886],\n",
       "          [ 0.06083751,  0.22193162,  0.16683716,  0.74703954, -0.10257913,\n",
       "            0.15653385, -0.42704787, -1.27649491,  0.3268093 ,  0.4322181 ,\n",
       "           -0.37108251,  1.13487731, -0.72718284],\n",
       "          [ 0.02287926, -0.09359193,  0.76638961,  0.73467938,  0.07747371,\n",
       "            0.18908126, -0.44389287, -0.99039823, -0.17395607,  0.07817448,\n",
       "            0.61514534,  0.60118992, -0.19366341],\n",
       "          [-0.15115138, -0.52427648, -0.71000897, -0.8531351 ,  0.9753877 ,\n",
       "           -0.25482609, -0.21903715, -0.62639768,  0.38874518, -0.80694892,\n",
       "           -0.10637014, -0.44773328,  0.19345125],\n",
       "          [-0.25540257, -0.59031609, -0.01409111,  0.21416594,  0.03325861,\n",
       "            0.15123595, -0.31716105, -0.18137058, -0.33623022, -0.17977658,\n",
       "           -0.40657314, -0.8631413 ,  0.08871307],\n",
       "          [-0.20089047, -0.81509917,  0.23139113, -0.45364918,  0.0259727 ,\n",
       "            0.36454528,  0.06449146,  0.56970034, -0.61741291,  0.20117082,\n",
       "           -0.34240505, -0.43539857, -0.28942483],\n",
       "          [-0.15577627,  0.02808267, -0.58257492,  0.45041324,  0.23283122,\n",
       "           -0.76812184,  0.7441261 ,  0.94794459,  0.58938979, -0.08996242,\n",
       "           -0.53537631,  0.52722586, -0.20158847],\n",
       "          [ 0.61122254,  0.10413749,  0.48831952,  0.1781832 ,  0.35328658,\n",
       "            0.00525001,  0.89293525,  0.06345605,  0.20099468,  0.94157535,\n",
       "           -0.67387953, -0.6352425 ,  0.48469835]]),\n",
       "   array([[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]])),\n",
       "  array([[  35.4606791 ,   35.90210313,   40.85013727, ...,   28.01933884,\n",
       "            43.45376328,   36.90171873],\n",
       "         [ -54.49286069,  -41.12770624,  -44.89327032, ...,   -6.05997246,\n",
       "           -56.78561053,  -49.48593836],\n",
       "         [ 187.22948877,  185.19331868,  185.92288297, ...,  173.09129855,\n",
       "           211.52648396,  196.41149142],\n",
       "         ...,\n",
       "         [-323.92249136, -316.82385956, -344.15535791, ..., -400.17489835,\n",
       "          -281.8836363 , -322.4274645 ],\n",
       "         [ -29.31811226,  -26.01240342,  -29.42035995, ...,   40.74315825,\n",
       "           -69.49203103,  -36.30738857],\n",
       "         [-172.39338521, -159.86075218, -172.46584231, ..., -151.46005384,\n",
       "          -195.89506682, -185.56579125]])),\n",
       " ((array([[ 35.4606791 ,  35.90210313,  40.85013727, ...,  28.01933884,\n",
       "            43.45376328,  36.90171873],\n",
       "          [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "             0.        ,   0.        ],\n",
       "          [187.22948877, 185.19331868, 185.92288297, ..., 173.09129855,\n",
       "           211.52648396, 196.41149142],\n",
       "          ...,\n",
       "          [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "             0.        ,   0.        ],\n",
       "          [  0.        ,   0.        ,   0.        , ...,  40.74315825,\n",
       "             0.        ,   0.        ],\n",
       "          [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "             0.        ,   0.        ]]),\n",
       "   array([[-0.74194839,  1.22925397, -0.26159561, -0.47273193,  1.21617532,\n",
       "            0.93635977,  1.181148  ,  0.57303296],\n",
       "          [-0.54468695,  1.20803115, -0.16950021,  0.50751799,  0.59909475,\n",
       "           -0.09803699,  0.38837789,  0.58325471],\n",
       "          [ 0.23807241, -0.69532211,  0.18862238,  0.8388801 , -0.43928329,\n",
       "           -0.09463719, -0.27521527,  1.16957708],\n",
       "          [ 0.42519654,  0.25770149, -0.48693768,  0.34105113, -0.42648542,\n",
       "            0.02013141, -0.40214437,  0.42781398],\n",
       "          [ 0.36466805, -0.1317397 ,  0.25045664, -0.6913128 , -0.94315411,\n",
       "            0.27789571,  0.10541357,  0.40162915]]),\n",
       "   array([[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]])),\n",
       "  array([[ -99.98025483,  -98.84934675,  -95.50961664, ...,  -64.37771039,\n",
       "           -97.02263838, -102.31427085],\n",
       "         [ -24.54160556,  -25.43080382,  -35.98130481, ...,   21.07229335,\n",
       "           -49.37947643,  -28.10368118],\n",
       "         [  87.57446956,   85.65268266,   74.18848654, ...,  110.50270117,\n",
       "            67.0092181 ,   87.63158914],\n",
       "         [ -58.27746977,  -57.76618316,  -61.21324317, ...,  -55.25692392,\n",
       "           -77.70774494,  -62.95618895],\n",
       "         [  23.71542767,   24.72025252,   37.23907251, ...,  -10.03748548,\n",
       "            55.00817208,   28.20354008]])),\n",
       " (array([[  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,  21.07229335,\n",
       "            0.        ,   0.        ],\n",
       "         [ 87.57446956,  85.65268266,  74.18848654, ..., 110.50270117,\n",
       "           67.0092181 ,  87.63158914],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [ 23.71542767,  24.72025252,  37.23907251, ...,   0.        ,\n",
       "           55.00817208,  28.20354008]]),\n",
       "  array([[ 3.37027566,  1.3356957 , -1.29092557,  1.57969958, -1.86097411]]),\n",
       "  array([[0.]]))]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "bb57d595-aff1-4eaf-839f-3d2acaa3b92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[ 35.4606791 ,  35.90210313,  40.85013727, ...,  28.01933884,\n",
       "           43.45376328,  36.90171873],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [187.22948877, 185.19331868, 185.92288297, ..., 173.09129855,\n",
       "          211.52648396, 196.41149142],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,  40.74315825,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ]]),\n",
       "  array([[-0.74194839,  1.22925397, -0.26159561, -0.47273193,  1.21617532,\n",
       "           0.93635977,  1.181148  ,  0.57303296],\n",
       "         [-0.54468695,  1.20803115, -0.16950021,  0.50751799,  0.59909475,\n",
       "          -0.09803699,  0.38837789,  0.58325471],\n",
       "         [ 0.23807241, -0.69532211,  0.18862238,  0.8388801 , -0.43928329,\n",
       "          -0.09463719, -0.27521527,  1.16957708],\n",
       "         [ 0.42519654,  0.25770149, -0.48693768,  0.34105113, -0.42648542,\n",
       "           0.02013141, -0.40214437,  0.42781398],\n",
       "         [ 0.36466805, -0.1317397 ,  0.25045664, -0.6913128 , -0.94315411,\n",
       "           0.27789571,  0.10541357,  0.40162915]]),\n",
       "  array([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]])),\n",
       " array([[ -99.98025483,  -98.84934675,  -95.50961664, ...,  -64.37771039,\n",
       "          -97.02263838, -102.31427085],\n",
       "        [ -24.54160556,  -25.43080382,  -35.98130481, ...,   21.07229335,\n",
       "          -49.37947643,  -28.10368118],\n",
       "        [  87.57446956,   85.65268266,   74.18848654, ...,  110.50270117,\n",
       "           67.0092181 ,   87.63158914],\n",
       "        [ -58.27746977,  -57.76618316,  -61.21324317, ...,  -55.25692392,\n",
       "          -77.70774494,  -62.95618895],\n",
       "        [  23.71542767,   24.72025252,   37.23907251, ...,  -10.03748548,\n",
       "           55.00817208,   28.20354008]]))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_forward[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ddc4f-306f-4f86-897b-ac08c0545f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
