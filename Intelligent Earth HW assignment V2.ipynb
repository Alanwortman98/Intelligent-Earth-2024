{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce5e4a5-9988-4500-836d-0a2e6b88100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ea4f4-f265-427d-89ff-45cfdcf96fc7",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02df2a2f-402c-40a2-87d0-686a63d0e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization is poor for relu\n",
    "\n",
    "def initialize_params(dims): # He initialization ; for ReLu and Leaky ReLu activations\n",
    "    params = {}\n",
    "    for i in range(1, len(dims)):\n",
    "        params.update({f'W{i}': np.random.randn(dims[i],dims[i-1]) * np.sqrt(2 / dims[i])}) # scaled initialization for ReLU\n",
    "        params.update({f'B{i}': np.zeros((dims[i], 1))})\n",
    "\n",
    "    assert(params['W' + str(i)].shape == (dims[i], dims[i-1])) # double check dimensions are correct\n",
    "    assert(params['B' + str(i)].shape == (dims[i], 1))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b48d0-817e-4dec-97c4-c35384069e88",
   "metadata": {},
   "source": [
    "### Define activation functions\n",
    "\n",
    "Using ReLu for this assignment. \n",
    "\n",
    "Several others are included which I would like to fill in later for my own understanding/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f7dcca-9f10-466d-b892-219da2d21541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "Z - pre activation parameter ; output of the linear layer\n",
    "\n",
    "Returns:\n",
    "A - post activation parameter ; of the same shape as Z\n",
    "cache - returns Z ; used during backpropagation\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z)) # the sigmoid function\n",
    "    dZ = dA * (s * (1-s)) # dA * the derivative of sigmoid. sigmoid derivative = (e ** -x) /(1 + e ** -X) = sig(x) * (1 - sig(x))\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    dZ[Z <= 0] = 0 # When z <= 0, you should set dz to 0 as well - think of what ReLu looks like. \n",
    "\n",
    "    assert (dZ.shape == Z.shape) # check if dZ has correct shape\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def tanh(Z):\n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    return A, cache\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    Z = cache\n",
    "    t = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z)) # the tanh function\n",
    "    dZ = dA * (1 - t ** 2) # dA * dervative of tanh. \n",
    "\n",
    "    assert (dZ.shape == Z.shape) # check if dZ has correct shape\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def softplus(Z):\n",
    "    A = np.log(1 + np.exp(Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softplus_backward(dA, cache): # define later\n",
    "    pass\n",
    "\n",
    "def softmax(Z): # really only used in the output layer\n",
    "    A = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.exp(Z - np.max(Z, axis=1, keepdims=True)).sum(axis=1, keepdims=True) # numerically stable softmax\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backeard(dA, cache): # define later\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c921f-0e58-40d6-bcca-e44f1eb82e17",
   "metadata": {},
   "source": [
    "### Feed Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f85be1-0e51-4148-a3c2-ffab888fe9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward (A, W, b): # linear transformation\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A - activations from previous layer: numpy array of shape (size of previous layer, number of examples)\n",
    "    W - weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b - bias vector: numpy array of shape (size of current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z - input of the activation function (pre-activation parameter)\n",
    "    cache - tuple: (A, W, b) ; used for computing backwards pass\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1])) # double check dimensions are correct\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483df950-4211-4026-8866-687283fe0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, weights, biases, activation): # linear transformation with activation function\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A_prev - activations from previous layer: numpy array of shape (size of previous layer, number of examples)\n",
    "    weights - weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    biases - bias vector: numpy array of shape (size of current layer, 1)\n",
    "    activation - the activation function to be used: string\n",
    "\n",
    "    Returns:\n",
    "    A_new - output of activation function: \n",
    "    cache - tuple: (linear cache, activation cache) ; used for computing backwards pass\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, weights, biases)\n",
    "\n",
    "    if activation == \"tanh\":\n",
    "        A_new, activation_cache = tanh(Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        A_new, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        A_new, activation_cache = relu(Z)\n",
    "\n",
    "    elif activation == \"softplus\":\n",
    "        pass\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A_new, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e19496f-0fdb-40a4-a66e-f325f89363b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(X, parameters): # feed forward\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X - data to be modeled: numpy array of shape (input size, number of examples)\n",
    "    parameters - python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "\n",
    "    Returns:\n",
    "    A_final - activations from final layer\n",
    "    caches - an indexed list of caches from every layer containing the previous activations, weights, biases, and pre-activation parameters \n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # number of layers in the model\n",
    "\n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[f'W{i}'], parameters[f'B{i}'], \"relu\") # hidden layers # note: relu is hard coded\n",
    "        caches.append(cache)\n",
    "\n",
    "    A_L, cache = linear_forward(A, parameters[f'W{L}'], parameters[f'B{L}']) # output layer # note: for regression, usually no activation (or a linear transformation) is used.\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(AL.shape == (1,X.shape[1])) # double check dimensions are correct\n",
    "\n",
    "    return A_L, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205b57e-7188-4610-8a9e-0957a46c9368",
   "metadata": {},
   "source": [
    "### Define some loss functions\n",
    "\n",
    "Will be using MSE in this assignment\n",
    "\n",
    "Several others are included which I would like to fill in later for my own understanding/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b06e4f-16c5-4568-abe0-7b29e5782337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "Returns:\n",
    "cost -- cross-entropy cost\n",
    "\"\"\"\n",
    "\n",
    "def binary_cross_entropy(): # log loss ; classification\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)) # using dot product instead of np.sum() becasue it's faster\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def categorical_cross_entropy(): # log loss ; classification\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * np.sum(np.mulitply(Y, np.log(AL))) # this is unfinished and I'm not sure if it's correct\n",
    "\n",
    "    return cost\n",
    "\n",
    "def neg_log_likelihood(): # for simple classifications\n",
    "    pass\n",
    "\n",
    "def MSE(AL, Y): # mean squared error ; regression\n",
    "    m = Y.shape[1]\n",
    "    cost = (1 / m) * np.dot((AL-Y).flatten(), (AL-Y).flatten()) # using dot product instead of np.sum() because it's faster\n",
    "                                                                # does anything need to be trasposed?\n",
    "    return cost\n",
    "\n",
    "def MAE(): # mean absolute error ; regression\n",
    "    pass\n",
    "\n",
    "def KL_divergence(): # Kullback-Leibler Divergence\n",
    "    pass\n",
    "\n",
    "def Huber(): # Huber loss ; regression\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e2882-d710-41a4-8e32-a458ceb541af",
   "metadata": {},
   "source": [
    "### Define a backprop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6fde9e-1931-42b0-9f36-64d9e6b4e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can the autodiff assignment be used here?\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dZ - Gradient of the cost with respect to the linear output (of the current layer)\n",
    "    cache - tuple ; (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev - Gradient of the cost with respect to the activation (of the previous layer) ; same shape as A_prev\n",
    "    dW - Gradient of the cost with respect to W (of the current layer) ; same shape as W\n",
    "    db - Gradient of the cost with respect to b (of the current layer) ; same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1] # is this the length of entries?\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ) # what's happening here?\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T) # what's happening here?\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims = True) # what's happening here?\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape) \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417f3eea-58af-45d6-8fe4-84f00817d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    activation function takes input Z and returns A and an activation cache (Z) ; \n",
    "    activation_backward function takes input dA and the activation cache (Z) and returns dZ.\n",
    "    \n",
    "    Args:\n",
    "    dA - post-activation gradient for current layer\n",
    "    cache - tuple ; (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation - activation function used in this layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev - Gradient of the cost with respect to the activation (of the previous layer) ; same shape as A_prev\n",
    "    dW - Gradient of the cost with respect to W (of the current layer) ; same shape as W\n",
    "    db - Gradient of the cost with respect to b (of the current layer) ; same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\": # relu activation takes input Z and returns A and an activation cache (Z) ; relu_backward takes input dA and the activation cache (Z) and returns dZ\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dA_prev, dW, b = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9724910-61a1-4a1d-bc53-0f59c2843f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    AL - probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y - true \"label\" vector\n",
    "    caches - list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "                how would this work for tanh? softplus?\n",
    "\n",
    "    Returns:\n",
    "    grads - dictionary ; gradients with respect to activations, weights, biases...\n",
    "                         grads[\"dA\" + str(l)] = ... \n",
    "                         grads[\"dW\" + str(l)] = ...\n",
    "                         grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    L = length(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    dAL = np.divide(Y, AL) - np.divide(1 - Y, 1 - AL) # initialize backprop\n",
    "\n",
    "    current_cache = caches[L-1] # Last Layer\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "        dAL, \n",
    "        current_cache, \n",
    "        activation = \"relu\") # note: relu is hard coded. This is the last (output) layer\n",
    "\n",
    "    for i in reversed(range(L-1)): # Loop from l=L-2 to l=0\n",
    "        current_cache = caches[i]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
    "            grads[\"dA\" + str(i+1)],\n",
    "            current_cache,\n",
    "            activation = \"relu\") # note: relu is hard coded. These are the hidden layers\n",
    "\n",
    "        grads[\"dA\" + str(i)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(i+1)] = dW_temp\n",
    "        grads[\"db\" + str(i+1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b8dab-f8d4-40d3-9525-724a78cf8be3",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "which gradient descent update rule to use? \n",
    "\n",
    "which learning rate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b3df7f-1267-4772-a9a4-d7d566938e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_params(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    generally we update by: new_params = params - grads * learning_rate\n",
    "    Args:\n",
    "    params - dict ; contains the (previous) parameters\n",
    "    grads - dict ; contains the graduients calculated from backprop\n",
    "    learning_rate - step size for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params - dict ; the updated parameters\n",
    "                parameters[\"W\" + str(l)] = ... \n",
    "                parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(params) // 2 # params dict contains entries for both weights and biases\n",
    "\n",
    "    for i in range(L):\n",
    "        params[\"W\" + str(i+1)] = params[\"W\" + str(i+1)] - grads[\"dW\" + str(i+1)] * learning_rate\n",
    "        params[\"b\" + str(i+1)] = params[\"b\" + str(i+1)] - grads[\"db\" + str(i+1)] * learning_rate\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09bb97b-d660-452c-a682-7818411d280a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
