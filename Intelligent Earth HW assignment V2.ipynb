{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce5e4a5-9988-4500-836d-0a2e6b88100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ea4f4-f265-427d-89ff-45cfdcf96fc7",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02df2a2f-402c-40a2-87d0-686a63d0e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization is poor for relu\n",
    "\n",
    "def initialize_params(dims): # He initialization ; for ReLu and Leaky ReLu activations\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dims - python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters - python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1]) \n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    params = {}\n",
    "    for i in range(1, len(dims)):\n",
    "        params.update({f'W{i}': np.random.randn(dims[i],dims[i-1]) * np.sqrt(2 / dims[i])}) # scaled initialization for ReLU\n",
    "        params.update({f'B{i}': np.zeros((dims[i], 1))})\n",
    "\n",
    "    assert(params['W' + str(i)].shape == (dims[i], dims[i-1])) # double check dimensions are correct\n",
    "    assert(params['B' + str(i)].shape == (dims[i], 1))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b48d0-817e-4dec-97c4-c35384069e88",
   "metadata": {},
   "source": [
    "### Define activation functions\n",
    "\n",
    "Using ReLu for this assignment. \n",
    "\n",
    "Several others are included which I would like to fill in later for my own understanding/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f7dcca-9f10-466d-b892-219da2d21541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "Z - pre activation parameter ; output of the linear layer\n",
    "\n",
    "Returns:\n",
    "A - post activation parameter ; of the same shape as Z\n",
    "cache - returns Z ; used during backpropagation\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z)) # the sigmoid function\n",
    "    dZ = dA * (s * (1-s)) # dA * the derivative of sigmoid. sigmoid derivative = (e ** -x) /(1 + e ** -X) = sig(x) * (1 - sig(x))\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    dZ[Z <= 0] = 0 # When z <= 0, you should set dz to 0 as well - think of what ReLu looks like. \n",
    "\n",
    "    assert (dZ.shape == Z.shape) # check if dZ has correct shape\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def tanh(Z):\n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    return A, cache\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    Z = cache\n",
    "    t = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z)) # the tanh function\n",
    "    dZ = dA * (1 - t ** 2) # dA * dervative of tanh. \n",
    "\n",
    "    assert (dZ.shape == Z.shape) # check if dZ has correct shape\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def softplus(Z):\n",
    "    A = np.log(1 + np.exp(Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softplus_backward(dA, cache): # define later\n",
    "    pass\n",
    "\n",
    "def softmax(Z): # really only used in the output layer\n",
    "    A = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.exp(Z - np.max(Z, axis=1, keepdims=True)).sum(axis=1, keepdims=True) # numerically stable softmax\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backeard(dA, cache): # define later\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c921f-0e58-40d6-bcca-e44f1eb82e17",
   "metadata": {},
   "source": [
    "### Feed Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f85be1-0e51-4148-a3c2-ffab888fe9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward (A, W, b): # linear transformation\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A - activations from previous layer: numpy array of shape (size of previous layer, number of examples)\n",
    "    W - weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b - bias vector: numpy array of shape (size of current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z - input of the activation function (pre-activation parameter)\n",
    "    cache - tuple: (A, W, b) ; used for computing backwards pass\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b  \n",
    "    cache = (A, W, b) \n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1])) # double check dimensions are correct (W rows, A columns)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483df950-4211-4026-8866-687283fe0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, weights, biases, activation): # linear transformation with activation function\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A_prev - activations from previous layer: numpy array of shape (size of previous layer, number of examples)\n",
    "    weights - weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    biases - bias vector: numpy array of shape (size of current layer, 1)\n",
    "    activation - the activation function to be used: string\n",
    "\n",
    "    Returns:\n",
    "    A_new - output of activation function: \n",
    "    cache - tuple: (linear cache, activation cache) ; used for computing backwards pass\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, weights, biases) # calculate pre-activation parameter from previous inputs, weight matrix and biases\n",
    "\n",
    "    if activation == \"tanh\":\n",
    "        A_new, activation_cache = tanh(Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        A_new, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        A_new, activation_cache = relu(Z)\n",
    "\n",
    "    elif activation == \"softplus\":\n",
    "        pass\n",
    "\n",
    "    assert (A_new.shape == (weights.shape[0] , A_prev.shape[1])) # (W rows, A cols)\n",
    "\n",
    "    cache = (linear_cache, activation_cache) # entry 1: tuple containg A, W, b ; entry 2: Z that was used in activation function\n",
    "\n",
    "    return A_new, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e19496f-0fdb-40a4-a66e-f325f89363b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(X, parameters): # feed forward\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X - data to be modeled: numpy array of shape (input size, number of examples)\n",
    "    parameters - python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "\n",
    "    Returns:\n",
    "    A_final - activations from final layer\n",
    "    caches - an indexed list of caches from every layer containing the previous activations, weights, biases, and pre-activation parameters as tuples \n",
    "    \"\"\"\n",
    "\n",
    "    caches = [] \n",
    "    A = X\n",
    "    L = len(parameters) // 2 # params accounts for weight matrices and bias vectors (2 things)\n",
    "\n",
    "    for i in range(1, L): # linear --> relu for layers 1-->(L-1)\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[f'W{i}'], parameters[f'B{i}'], \"relu\") # hidden layers # note: relu is hard coded\n",
    "        caches.append(cache)\n",
    "\n",
    "    # linear transformation for last layer (regression model)\n",
    "    A_L, linear_cache = linear_forward(A, parameters[f'W{L}'], parameters[f'B{L}']) # output layer # note: for regression, usually no activation (or a linear transformation) is used.\n",
    "    caches.append((linear_cache, None))\n",
    "\n",
    "    assert(A_L.shape == (1, X.shape[1])) # double check dimensions are correct\n",
    "\n",
    "    return A_L, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205b57e-7188-4610-8a9e-0957a46c9368",
   "metadata": {},
   "source": [
    "### Define some loss functions\n",
    "\n",
    "Will be using MSE in this assignment  \n",
    "Several others are included which I would like to fill in later for my own understanding/research  \n",
    "### Dot Product Formula\n",
    "\n",
    "The dot product of two vectors $\\mathbf{A}$ and $\\mathbf{B}$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i \\cdot B_i\n",
    "$$\n",
    "\n",
    "### Mean Squared Error (MSE) Formula\n",
    "\n",
    "The Mean Squared Error (MSE) is calculated as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{m} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### Gradient of MSE\n",
    "\n",
    "The gradient of MSE is calculated as \n",
    "\n",
    "$$\n",
    "dA_L = \\frac{2}{m} \\ (A_L-Y)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "$A_L$ is the matrix of predictions (output activations)   \n",
    "$Y$ is the matrix of true labels (ground truth)   \n",
    "$m$ is the number of examples (data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c3b06e4f-16c5-4568-abe0-7b29e5782337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "Returns:\n",
    "cost -- cross-entropy cost\n",
    "\"\"\"\n",
    "\n",
    "def binary_cross_entropy(AL, Y): # log loss ; classification\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)) # using dot product instead of np.sum() becasue it's faster\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def categorical_cross_entropy(AL, Y): # log loss ; classification\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m) * np.sum(np.mulitply(Y, np.log(AL))) # this is unfinished and I'm not sure if it's correct\n",
    "\n",
    "    return cost\n",
    "\n",
    "def neg_log_likelihood(): # for simple classifications\n",
    "    pass\n",
    "\n",
    "def MSE(AL, Y): # mean squared error ; regression\n",
    "    #m = Y.shape[1]\n",
    "    m = Y.shape[0]\n",
    "    cost = (1 / m) * np.dot((AL-Y).flatten(), (AL-Y).flatten()) # using dot product instead of np.sum() because it's faster\n",
    "                                                                # does anything need to be trasposed?\n",
    "    return cost\n",
    "\n",
    "def MAE(): # mean absolute error ; regression\n",
    "    pass\n",
    "\n",
    "def KL_divergence(): # Kullback-Leibler Divergence\n",
    "    pass\n",
    "\n",
    "def Huber(): # Huber loss ; regression\n",
    "    pass\n",
    "\n",
    "def MSE_grad(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    dAL = (2/m) * (AL - Y)\n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e2882-d710-41a4-8e32-a458ceb541af",
   "metadata": {},
   "source": [
    "### Define a backprop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2e6fde9e-1931-42b0-9f36-64d9e6b4e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can the autodiff assignment be used here?\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dZ - Gradient of the cost with respect to the linear output (of the current layer)\n",
    "    cache - tuple ; (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev - Gradient of the cost with respect to the activation (of the previous layer) ; same shape as A_prev\n",
    "    dW - Gradient of the cost with respect to W (of the current layer) ; same shape as W\n",
    "    db - Gradient of the cost with respect to b (of the current layer) ; same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1] # is this the length of entries?\n",
    "\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ) # what's happening here?\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T) # what's happening here?\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims = True) # what's happening here?\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape) \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417f3eea-58af-45d6-8fe4-84f00817d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    activation function takes input Z and returns A and an activation cache (Z) ; \n",
    "    activation_backward function takes input dA and the activation cache (Z) and returns dZ.\n",
    "    \n",
    "    Args:\n",
    "    dA - post-activation gradient for current layer\n",
    "    cache - tuple ; (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation - activation function used in this layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev - Gradient of the cost with respect to the activation (of the previous layer) ; same shape as A_prev\n",
    "    dW - Gradient of the cost with respect to W (of the current layer) ; same shape as W\n",
    "    db - Gradient of the cost with respect to b (of the current layer) ; same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\": # relu activation takes input Z and returns A and an activation cache (Z) ; relu_backward takes input dA and the activation cache (Z) and returns dZ\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dA_prev, dW, b = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e4218-aff6-4af4-aa93-420fd1f7945a",
   "metadata": {},
   "source": [
    "### Updated linear_activation_backward() function which accounts for not having an activation cache in the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b538d9da-2c47-42a1-8082-caf421abbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation = None):\n",
    "    \"\"\"\n",
    "    activation function takes input Z and returns A and an activation cache (Z) ; \n",
    "    activation_backward function takes input dA and the activation cache (Z) and returns dZ.\n",
    "    \n",
    "    Args:\n",
    "    dA - post-activation gradient for current layer\n",
    "    cache - tuple ; (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation - activation function used in this layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev - Gradient of the cost with respect to the activation (of the previous layer) ; same shape as A_prev\n",
    "    dW - Gradient of the cost with respect to W (of the current layer) ; same shape as W\n",
    "    db - Gradient of the cost with respect to b (of the current layer) ; same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\": # relu activation takes input Z and returns A and an activation cache (Z) ; relu_backward takes input dA and the activation cache (Z) and returns dZ\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    elif activation is None: # No activation function ; final layer for regression\n",
    "        dZ = dA # Z = A and dZ = dA for the final layer if no activation is used\n",
    "\n",
    "    dA_prev, dW, b = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b9724910-61a1-4a1d-bc53-0f59c2843f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    AL - probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y - true \"label\" vector\n",
    "    caches - list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "                how would this work for tanh? softplus?\n",
    "\n",
    "    Returns:\n",
    "    grads - dictionary ; gradients with respect to activations, weights, biases...\n",
    "                         grads[\"dA\" + str(l)] = ... \n",
    "                         grads[\"dW\" + str(l)] = ...\n",
    "                         grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    # m = AL.shape[0]\n",
    "    Y = Y[0]\n",
    "    # Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # initialize backprop, note: this is the gradient of cross entropy loss\n",
    "    dAL = MSE_grad(AL, Y) # gradient for mean-squared-error\n",
    "\n",
    "    current_cache = caches[L-1] # Last Layer\n",
    "    #current_cache = caches[L-2] # b/c no activation cache in last layer since no activation function was used\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "        dAL, \n",
    "        current_cache, \n",
    "        activation = None) # note: no activation in the final layer for regression ; this is hard coded\n",
    "\n",
    "    for i in reversed(range(L-1)): # Loop from l=L-2 to l=0\n",
    "        current_cache = caches[i]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
    "            grads[\"dA\" + str(i+1)],\n",
    "            current_cache,\n",
    "            activation = \"relu\") # note: relu is hard coded ; these are the hidden layers\n",
    "\n",
    "        grads[\"dA\" + str(i)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(i+1)] = dW_temp\n",
    "        grads[\"db\" + str(i+1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6eb20f8b-43f0-4432-9725-e9cd68a4d064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 1)\n",
      "(404,)\n"
     ]
    }
   ],
   "source": [
    "print(A_L.T.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b8dab-f8d4-40d3-9525-724a78cf8be3",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "which gradient descent update rule to use? \n",
    "\n",
    "which learning rate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "20b3df7f-1267-4772-a9a4-d7d566938e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_params(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    generally we update by: new_params = params - grads * learning_rate\n",
    "    Args:\n",
    "    params - dict ; contains the (previous) parameters\n",
    "    grads - dict ; contains the graduients calculated from backprop\n",
    "    learning_rate - step size for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params - dict ; the updated parameters\n",
    "                parameters[\"W\" + str(l)] = ... \n",
    "                parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(params) // 2 # params dict contains entries for both weights and biases\n",
    "\n",
    "    for i in range(L):\n",
    "        params[\"W\" + str(i+1)] = params[\"W\" + str(i+1)] - grads[\"dW\" + str(i+1)] * learning_rate\n",
    "        params[\"B\" + str(i+1)] = params[\"B\" + str(i+1)] - grads[\"db\" + str(i+1)] * learning_rate\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c3444-9a23-4686-ba11-fbc9142a3a06",
   "metadata": {},
   "source": [
    "### Import a Dataset for regression\n",
    "\n",
    "There are 14 attributes in each case of the dataset. They are:  \n",
    "CRIM - per capita crime rate by town  \n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.  \n",
    "INDUS - proportion of non-retail business acres per town.  \n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)  \n",
    "NOX - nitric oxides concentration (parts per 10 million)  \n",
    "RM - average number of rooms per dwelling  \n",
    "AGE - proportion of owner-occupied units built prior to 1940  \n",
    "DIS - weighted distances to five Boston employment centres  \n",
    "RAD - index of accessibility to radial highways  \n",
    "TAX - full-value property-tax rate per ${$10,000} $  \n",
    "PTRATIO - pupil-teacher ratio by town  \n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town  \n",
    "LSTAT - % lower status of the population  \n",
    "MEDV - Median value of owner-occupied homes in \\$1000's  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c09bb97b-d660-452c-a682-7818411d280a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Housing = pd.read_csv(\"BostonHousing.csv\")\n",
    "Housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9478135-2927-4424-817e-ef233a832423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data set into numpy. Goal is to predict the age of a person based on the other attributes for housing\n",
    "\n",
    "features = Housing.columns.drop('age').to_list() \n",
    "\n",
    "inputs = Housing[features].to_numpy() # input features\n",
    "targets = Housing.age.to_numpy() # target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aa479296-1d8c-42fd-a0c3-a694f811ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set up into training sets and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, targets, test_size = 0.2, random_state = 0)\n",
    "\n",
    "Y_train = Y_train.reshape(1,-1) # reshape Y_train to be the same dimensions as A_L (1,404)\n",
    "X_train = X_train.T # transpose to have 13 rows (for features) and 406 columns (for data points)\n",
    "X_test = X_test.T # this makes things compatible with the layout of the weights matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31955c4-dbf0-4655-a61d-60004da27eb7",
   "metadata": {},
   "source": [
    "### Set up neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "633d1d86-c663-46b1-a243-c54dec25111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number layers and nodes in each\n",
    "# initialize parameters\n",
    "\n",
    "dimensions = [13,8,5,1] # the layout of the neural network\n",
    "# input layer must have dims of # of features. output layer must have dim of 1 for regression (multiple for multiclass classification)\n",
    "\n",
    "params = initialize_params(dimensions) # initial weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "10d7e52b-a735-40bc-9a7e-3c89e2ad51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_L, cache_forward = model_forward(X_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b40e03a6-b9f2-472e-91f4-1dc930e66252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283894858.8188946\n"
     ]
    }
   ],
   "source": [
    "assert A_L.shape == Y_train.shape\n",
    "error = MSE(A_L, Y_train)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6d5b9ad0-804c-485a-bb61-b0147902dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dAL = MSE_grad(A_L, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8eb3d8b9-1456-4c89-a2db-05f6107e97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = model_backward(A_L, Y_train, cache_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "95732fc0-1296-465a-863d-cf76e4333e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = update_params(params, grads, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6facb38-d4d9-4a06-98f2-50515d1d231c",
   "metadata": {},
   "source": [
    "### Loop through the process to learn the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad3ec3-672d-453f-8df4-b8b6d59eab7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
